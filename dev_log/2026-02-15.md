# Dev Log - 2026-02-15

## Module 3: Chunks2SKUs - Implementation Complete

### Overview
Implemented the knowledge extraction module that processes chunks from Module 2 and extracts 4 types of Standard Knowledge Units (SKUs): Factual, Relational, Procedural, and Meta.

### Core Features Implemented

**1. Four Knowledge Extractors (Sequential Processing)**
- **Factual Extractor**: Isolated processing. Extracts standalone facts, definitions, statistics → new SKU folders
- **Relational Extractor**: Read-and-update mode. Maintains label_tree.json (hierarchical categories) + glossary.json (term definitions)
- **Procedural Extractor**: Isolated processing. Extracts workflows, step-by-step processes → Claude Code skill format
- **Meta Extractor**: Read-and-update mode. Maintains mapping.md (SKU router) + eureka.md (creative insights)

**2. Dual-Prompt Meta Extraction**
Split into two separate LLM calls with different temperatures:
- `mapping.md`: Temperature 0.2 for accuracy and precision
- `eureka.md`: Temperature 0.7 for creativity and unexpected connections

**3. Claude Code Skill Format**
Procedural knowledge output as SKILL.md with YAML frontmatter:
```yaml
---
name: g-sib-score-calculation
description: Calculate systemic importance score using indicator-based approach
---
# G-SIB Score Calculation
## Steps
1. Collect indicator data...
```

### Project Structure
```
src/chunks2skus/
├── cli.py                    # CLI: chunks2skus
├── config.py                 # Settings (EXTRACTION_MODEL, output dirs)
├── pipeline.py               # Main orchestration
├── router.py                 # Routes chunks to extractors
├── extractors/
│   ├── base.py               # Abstract BaseExtractor
│   ├── factual_extractor.py  # Isolated → new SKUs
│   ├── relational_extractor.py # Read-and-update → label_tree + glossary
│   ├── procedural_extractor.py # Isolated → skill folders
│   └── meta_extractor.py     # Read-and-update → mapping.md + eureka.md
├── schemas/
│   ├── sku.py                # SKUHeader, SKUType, LabelTree, Glossary
│   └── index.py              # SKUsIndex
└── utils/
    ├── logging_setup.py      # Dual-format logging
    └── llm_client.py         # OpenAI client for SiliconFlow GLM-5
```

### Output Structure
```
output/skus/
├── factual/
│   ├── sku_001/
│   │   ├── header.md         # Minimal metadata
│   │   └── content.md        # Factual knowledge
│   └── ...
├── relational/
│   ├── header.md
│   ├── label_tree.json       # Multi-level hierarchy
│   └── glossary.json         # Term definitions
├── procedural/
│   ├── skill_001/
│   │   └── SKILL.md          # Claude skill format
│   └── ...
└── meta/
    ├── header.md
    ├── mapping.md             # SKU router
    └── eureka.md              # Creative insights (append-only)
```

### Configuration (.env)
```bash
EXTRACTION_MODEL=Pro/zai-org/GLM-5
SKUS_OUTPUT_DIR=./output/skus
```

### Stress Test Results

**Basel Framework (21 chunks, ~863K tokens)**
- Output: 420 total SKUs
  - 84 Factual SKUs
  - 21 Procedural skills
  - 1 Relational knowledge base (label_tree + glossary)
  - 1 Meta knowledge (mapping + eureka)
- Processing: ~2 hours
- One timeout on relational extraction (chunk 20) - gracefully handled

### Key Design Decisions

| Decision | Choice | Rationale |
|----------|--------|-----------|
| Processing order | Factual → Relational → Procedural → Meta | Meta needs all SKU paths for mapping |
| Cross-chunk context | Isolated for F/P, Read-and-update for R/M | Relational/Meta accumulate knowledge |
| LLM model | GLM-5 | User specification |
| Skill format | Claude Code skill-creator format | Direct integration with Claude Code |
| Meta temperatures | 0.2 mapping / 0.7 eureka | Accuracy vs creativity tradeoff |
| eureka.md format | Append-only bullets | Preserves all creative insights |

### Eureka Insights Sample (from Basel Framework)

The eureka.md captured 63 creative insights including:
- [Feature] "Compliance module picker" for banks to selectively enable regulatory modules
- [Design] Hierarchical section numbering enables deep-linking via URL fragments
- [Insight] "Diversification incentive" - CDS indexes get higher correlation factor (80% vs 50%)
- [Feature] "Capital health dashboard" with color-coded zones for buffer quartiles

### CLI Commands
```bash
chunks2skus run                    # Process all chunks
chunks2skus run -v                 # Verbose mode
chunks2skus extract-chunk <path>   # Process single chunk
chunks2skus show-index             # Display SKUs index summary
chunks2skus init                   # Create output directories
```

### Issues & Fixes

**1. Verbose Flag Position**
- Error: `chunks2skus run -v` gave "No such option: -v"
- Fix: Moved verbose to parent group, use `chunks2skus -v run`

**2. LLM Timeout Handling**
- One relational extraction timed out on chunk 20
- System continued gracefully due to try/except error handling
- No data loss - previous chunks already saved

**3. Meta Extractor Refactoring**
- Initial design: Single prompt for both mapping + eureka
- Problem: Different tasks need different temperature settings
- Fix: Split into two separate prompts with tailored temperatures and system prompts

### Schemas

**SKUHeader (minimal metadata)**
```python
class SKUHeader(BaseModel):
    name: str
    classification: SKUType  # factual/relational/procedural/meta
    character_count: int
    source_chunk: str
    description: str
```

**LabelTree (hierarchical categories)**
```python
class LabelNode(BaseModel):
    name: str
    children: list["LabelNode"] = []

class LabelTree(BaseModel):
    roots: list[LabelNode] = []
```

**GlossaryEntry (term definitions)**
```python
class GlossaryEntry(BaseModel):
    term: str
    definition: str
    labels: list[str] = []      # Links to label_tree
    source_chunk: str
    related_terms: list[str] = []
```

### Next Steps
- ~~Consider postprocessing: bucketing, duplicates/contradiction handling, confidence scoring~~ → Done (2026-02-16)
- Module 4: SKUs2Workspace (Final workspace assembly)

### Test Data Location
`test_data/` (gitignored)
- `basel_chunks/` - 22 chunks from Basel Framework
- `basel_skus/` - Extracted SKUs (420 total)

---

# Dev Log - 2026-02-16

## Module 3 Postprocessing: Bucketing, Dedup, Proofreading

### Overview
Implemented 3-step postprocessing pipeline inside `src/chunks2skus/postprocessors/`. This refines raw SKU quality after extraction: groups similar SKUs, removes duplicates, and assigns web-grounded confidence scores.

### New Files (10)
```
src/chunks2skus/
├── postprocessors/
│   ├── __init__.py
│   ├── base.py               # BasePostprocessor (load/save index, shared dirs)
│   ├── bucketing.py           # TF-IDF + label Jaccard + bge-m3 → agglomerative clustering
│   ├── dedup.py               # Two-tier: quick header scan → deep content read
│   ├── proofreading.py        # Bipolar: source penalty + web confidence
│   └── pipeline.py            # PostprocessingPipeline orchestrator
├── utils/
│   ├── embedding_client.py    # SiliconFlow bge-m3 (auto-batching at 64)
│   ├── jina_client.py         # Jina s.jina.ai search (rate-limited)
│   └── token_utils.py         # tiktoken cl100k_base wrapper
└── schemas/
    └── postprocessing.py      # Bucket, DedupReport, ConfidenceReport
```

### Modified Files (6)
- `config.py` — 7 new settings (bucket tokens, embedding model, similarity weights, dedup model, jina key) + `postprocessing_dir` property
- `schemas/sku.py` — `SKUHeader.confidence: Optional[float]`, updated `to_markdown()`
- `schemas/index.py` — `SKUEntry.confidence`, `SKUsIndex.remove_sku()` method
- `schemas/__init__.py` — exports new postprocessing schemas
- `cli.py` — `postprocess` command group with `all`, `bucket`, `dedup`, `proof` subcommands
- `pyproject.toml` — added `scikit-learn>=1.3.0`, `scipy>=1.11.0`

### Step 1: Bucketing
Groups factual and procedural SKUs by similarity into token-limited buckets (≤100K).

Three-aspect weighted similarity:
| Aspect | Weight | Method |
|--------|--------|--------|
| Literal | 0.2 | TF-IDF on descriptions → cosine similarity (sklearn) |
| Label | 0.3 | Jaccard on label paths from label_tree.json |
| Vector | 0.5 | bge-m3 embeddings → cosine similarity |

Recursive splitting via agglomerative clustering (scipy, average linkage).

**Test result** (Basel, 380 SKUs): 2 buckets (1 factual: 300 SKUs/37K tokens, 1 procedural: 80 SKUs/38K tokens). Both under 100K so no splitting needed — correct.

**Bug fix during testing**: SiliconFlow embedding API has batch limit of 64. Added auto-batching in `embedding_client.py`.

### Step 2: Dedup (Two-Tier)
Tier 1 (cheap model, headers only) → Tier 2 (GLM-5, full content, flagged pairs only).

**Test result** (Basel): Tier 1 flagged 23 pairs in procedural bucket. Tier 2 deep-read all 23 and kept all — no true duplicates in well-structured Basel content. Conservative by design.

**Bug fixes during testing**:
- Added sub-batching for Tier 1 scan (80 headers per batch) — 300 headers in one prompt caused truncated JSON
- Added counter tracking for `total_kept`/`total_deleted` in report

### Step 3: Proofreading — Bipolar Confidence Design

**Key design decision**: Source chunk alignment ≠ knowledge confidence. High alignment just proves extraction was faithful, not that the knowledge is externally valid.

**Bipolar scoring**:
- Source check is **penalty only** — faithful extraction → no effect, contradiction/hallucination → hard penalty (0.2-0.5)
- Web search is **the real confidence signal** — does the claim hold up against independent sources?
- Final: `confidence = web_confidence - source_penalty`

**Test results (Basel, 5 SKUs)**:

| SKU | Old (source boosts) | Bipolar | Why different |
|-----|-------------------|---------|---------------|
| sku_001 (Basel definition) | 0.98 | 0.90 | Web corroborates, no penalty |
| sku_002 (BCBS membership) | 0.95 | 0.75 | Niche topic, partial web support |
| sku_003 (Document metadata) | 1.00 | 0.65 | Metadata can't be externally verified |
| sku_004 (Framework structure) | — | 0.85 | Web confirms Basel sections |
| sku_005 (Scope topics) | — | 0.50 | Internal section codes, little web evidence |

Scores now reflect external verifiability. Document-specific metadata correctly scores lower than externally verifiable definitions.

**Bug fix**: Added `--chunks-dir` / `-c` CLI option and threaded through pipeline/postprocessor so proofreading can find source chunks from a custom path.

### Configuration Added (.env)
```bash
MAX_BUCKET_TOKENS=100000
EMBEDDING_MODEL=Pro/BAAI/bge-m3
SIMILARITY_WEIGHT_LITERAL=0.2
SIMILARITY_WEIGHT_LABEL=0.3
SIMILARITY_WEIGHT_VECTOR=0.5
DEDUP_SCAN_MODEL=Qwen/Qwen3-VL-235B-A22B-Instruct
JINA_API_KEY=...
```

### CLI Commands Added
```bash
chunks2skus postprocess all -s <skus_dir> -c <chunks_dir>
chunks2skus postprocess bucket -s <skus_dir>
chunks2skus postprocess dedup -s <skus_dir>
chunks2skus postprocess proof -s <skus_dir> -c <chunks_dir>
```

### Issues & Fixes Summary
1. **Embedding batch limit** — API rejects >64 inputs → auto-batch at 64
2. **Tier 1 header overflow** — 300 headers truncate JSON → sub-batch at 80
3. **Dedup counters not tracked** — `total_kept` always 0 → count after each action
4. **Source chunks not found** — hardcoded `settings.chunks_dir` → configurable `--chunks-dir`
5. **Source alignment inflating scores** — bipolar redesign separates penalty from signal

### Next Steps
- Module 4: SKUs2Workspace (Final workspace assembly)
