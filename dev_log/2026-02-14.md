# Dev Log - 2026-02-14

## Module 2: Markdown2Chunks - Implementation Complete

### Overview
Implemented the smart chunking module that processes markdown files too long for LLMs by splitting them into manageable chunks (max 100K tokens).

### Core Features Implemented

**1. Two Chunking Strategies**
- **Header Chunker ("Peeling Onion")**: Primary method for structured markdown. Splits hierarchically by header levels (H1 → H2 → H3).
- **LLM Chunker ("Driving Wedges")**: Fallback for plain text. Uses SiliconFlow GLM-4.7 to identify cut points with K nearest tokens, then Levenshtein matching to locate exact positions.

**2. Rolling Context Window**
- For documents exceeding 100K tokens, slides through in windows
- Each window sent to LLM to identify cut points
- Process repeats until entire document is chunked

**3. Token Estimation**
- Using tiktoken library (cl100k_base encoding)
- Runs locally, no API calls needed
- Fast and accurate for most modern LLMs

### Project Structure
```
src/markdown2chunks/
├── cli.py                    # CLI: md2chunks
├── config.py                 # Settings (MAX_TOKEN_LENGTH, K_NEAREST_TOKENS)
├── pipeline.py               # Main orchestration
├── router.py                 # Routes markdown vs JSON
├── chunkers/
│   ├── base.py
│   ├── header_chunker.py     # Header-based chunking
│   └── llm_chunker.py        # LLM-based with regex fallback
├── utils/
│   ├── token_estimator.py    # tiktoken wrapper
│   ├── levenshtein.py        # Fuzzy string matching
│   ├── markdown_utils.py     # Header parsing
│   └── logging_setup.py
└── schemas/
    ├── chunk.py              # Chunk + ChunkMetadata
    └── index.py              # ChunksIndex
```

### Configuration (.env)
```bash
MAX_TOKEN_LENGTH=100000       # Max tokens per chunk
K_NEAREST_TOKENS=50           # Tokens around cut point for LLM
CHUNKING_MODEL=Pro/zai-org/GLM-4.7
COMPLEX_MODEL=Pro/zai-org/GLM-5
```

### Stress Test Results

**Basel Framework PDF (2000 pages)**
- Input: 9.8MB PDF → 3.45M chars → 781K tokens
- Output: 21 chunks
- Time: ~47 minutes
- Header chunker found 13 sections, 2 oversized (294K + 480K tokens)
- LLM chunker re-chunked oversized sections

**Phil Gordon小绿皮书 (Chinese poker book)**
- Input: 111K chars → 87K tokens
- With 30K limit test: Split into 3 chunks (~30K each)

### Issues & Fixes

**1. LLM Context Window Confusion**
- Initially thought we needed separate LLM_WINDOW_TOKENS < MAX_TOKEN_LENGTH
- User clarified: GLM-4.7 supports 100K context, just slow
- Reverted to single MAX_TOKEN_LENGTH=100K

**2. Malformed JSON from LLM**
- LLM frequently returned invalid JSON (unquoted keys, single quotes)
- Added `_parse_llm_response()` with regex fallback
- Extracts tokens_before/tokens_after/chunk_title even from malformed responses
- Tested with 4 malformed patterns - all pass

**3. Model Names**
- Corrected from Qwen2.5 to user-specified GLM models:
  - `Pro/zai-org/GLM-4.7` for chunking
  - `Pro/zai-org/GLM-5` for complex tasks

### CLI Commands
```bash
md2chunks run                 # Process all markdown from module 1
md2chunks chunk-file X        # Chunk single file
md2chunks estimate-tokens X   # Show token count
```

### Output Format
- Each chunk: Markdown file with YAML frontmatter (title, source, chunk#, tokens, method)
- Index: `chunks_index.json` with all chunk metadata
- JSON pass-through: Copied unchanged to `passthrough/` directory

### Dependencies Added
```
tiktoken>=0.7.0
python-Levenshtein>=0.25.0
openai>=1.0.0
```

### Next Steps
- Module 3: Chunks2SKUs (Knowledge extraction)
- Consider adding LLM response logging for debugging
- Optimize LLM call speed (batch processing?)

### TODO
- [ ] Test nested folder structure in input/ - verify walk_directory handles multiple layers of subfolders and flattens output correctly

### Test Data Location
`test_data/` (gitignored)
- `Phil_Gordon小绿皮书.md` - parsed PDF
- `BaselFramework.md` - 2000-page stress test
- `chunks/`, `chunks_30k/`, `basel_chunks/` - chunking outputs
