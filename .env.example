# ===========================================
# Anything2Workspace - Environment Variables
# ===========================================

# --- I/O Paths ---
INPUT_DIR=./input
OUTPUT_DIR=./output
LOG_DIR=./logs

# --- API Keys ---
# SiliconFlow API key (for LLM features in later modules)
SILICONFLOW_API_KEY=

# MinerU API key (for complex/scanned PDF parsing)
MINERU_API_KEY=

# FireCrawl API key (for website crawling)
FIRECRAWL_API_KEY=

# --- MinerU Configuration (disabled, replaced by PaddleOCR-VL) ---
MINERU_API_ENDPOINT=https://mineru.net/api/v4/extract/task
MAX_PDF_SIZE_MB=10
MIN_VALID_CHARS=500

# --- PaddleOCR-VL Configuration (scanned PDF OCR fallback) ---
# Two backends: SiliconFlow API (default) or local mlx-vlm server
# SiliconFlow: leave OCR_BASE_URL empty, uses SILICONFLOW_BASE_URL + SILICONFLOW_API_KEY
# Local: pip install mlx-vlm, then: cd /tmp && python -m mlx_vlm.server --port 8080 --trust-remote-code
PADDLEOCR_MODEL=PaddlePaddle/PaddleOCR-VL-1.5
OCR_DPI=150
OCR_PAGE_TIMEOUT=60
# OCR_BASE_URL=                 # Empty = SiliconFlow API; http://localhost:8080 = local mlx-vlm
# For local deployment, also set:
# PADDLEOCR_MODEL=mlx-community/PaddleOCR-VL-1.5-8bit
# OCR_PAGE_TIMEOUT=120

# --- Processing Configuration ---
RETRY_COUNT=1
RETRY_DELAY_SECONDS=2

# --- Logging ---
# Options: DEBUG, INFO, WARNING, ERROR
LOG_LEVEL=INFO
# Options: json, text, both
LOG_FORMAT=both

# --- Language ---
# Output language for LLM prompts (en = English, zh = Chinese)
LANGUAGE=en

# ===========================================
# Module 2: Markdown2Chunks Configuration
# ===========================================

# --- Chunking Configuration ---
# Maximum tokens per chunk (also Rolling Context Window size)
MAX_TOKEN_LENGTH=100000
# Number of tokens around cut point for LLM output
K_NEAREST_TOKENS=50

# --- LLM Configuration (SiliconFlow) ---
SILICONFLOW_BASE_URL=https://api.siliconflow.cn/v1
# Model for chunking decisions (fast model)
CHUNKING_MODEL=Pro/zai-org/GLM-4.7
# Model for complex tasks
COMPLEX_MODEL=Pro/zai-org/GLM-5

# ===========================================
# Module 3: Chunks2SKUs
# ===========================================

# --- Extraction ---
EXTRACTION_MODEL=Pro/zai-org/GLM-5

# --- Postprocessing: Bucketing ---
MAX_BUCKET_TOKENS=100000
EMBEDDING_MODEL=Pro/BAAI/bge-m3
SIMILARITY_WEIGHT_LITERAL=0.2
SIMILARITY_WEIGHT_LABEL=0.3
SIMILARITY_WEIGHT_VECTOR=0.5

# --- Postprocessing: Dedup ---
DEDUP_SCAN_MODEL=Qwen/Qwen3-VL-235B-A22B-Instruct

# --- Postprocessing: Proofreading ---
JINA_API_KEY=

# ===========================================
# Module 4: SKUs2Workspace
# ===========================================

WORKSPACE_DIR=./workspace
CHATBOT_MODEL=Pro/zai-org/GLM-5
MAX_CHAT_ROUNDS=5
CHATBOT_TEMPERATURE=0.4
CHATBOT_MAX_TOKENS=8000

# ===========================================
# Bilibili Parser (Module 1)
# ===========================================

# Cookie authentication (Bilibili requires cookies to avoid HTTP 412)
# Option A: Netscape cookie file exported from browser
# BILIBILI_COOKIES_FILE=./cookies.txt
# Option B: Extract cookies from browser automatically (chrome/firefox/safari/edge)
BILIBILI_COOKIES_FROM_BROWSER=chrome

# Whisper model for transcription fallback (when no CC subtitles)
# Requires: pip install faster-whisper (and ffmpeg for audio extraction)
WHISPERX_MODEL=small
